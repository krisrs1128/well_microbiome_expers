\documentclass{article}
\usepackage{natbib}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{graphicx}
\input{preamble.tex}

\title{Multitable Data Analysis for the Microbiome}
\author{Kris Sankaran}

\begin{document}
\maketitle

The simultaneous study of multiple measurement types is a commonly encoutered
problem in practical data analysis. It is especially common in microbiome
research, where several sources of data -- for example, 16s, metagenomic,
metabolomic, or transcriptomic data -- can be collected on the same physical
samples\citep{Franzosa2015, McHardy2013}. There has been a proliferation of
proposals for analyzing this kind of data, as is often the case when new (or
more cheaply available) data sources makes inquiry into certain scientific
questions all of a sudden possible \citep{Fukuyama2017, Rahnavard2017,
  Chaudhary2017, Chalise2017}.

However, stepping back from the rush of new methods for multiple table analysis
in the microbiome literature, it is worthwhile to recognize the broader
landscape of multiple table methods, as they have been relevant in problem
domains ranging from economics to robotics to computational biology. Of course,
there is no unique optimal algorithm to use across domains -- different
instances of the multiple table problem possess specific structure or variation
that are worth incorporating in methodology.

Our purpose here is not to develop new algorithms, but rather to (1) distill the
relevant themes across different analysis approaches and (2) provide concrete
workflows for approaching analysis, as a function of ultimate analysis goals and
characteristics (heterogeneity, dimensionality, sparsity, ...) of the data.
Towards the second goal, code for all analysis and figures are available.

First, though, why does anyone bother collecting multiple sources of data, and
why can't these sources simply be combined into a single, unified table for
subsequent analysis? One answer is simply that it's now cheaper and easier to
collect multitable data, but a more satisfactory explanation is that many
scientific problems can only be answered by collecting several complementary
measurement types. Indeed, the situation is analogous to using many types of
sensors to study a single system from many perspectives. Further, while in
certain supervised problems, it is enough to predict a single measurement of
interest, with other sources simply collected to provide better features, there
are often an additional relational components to the analysis: how do different
types of measurements covary with one another? Here, it is of interest to
provide a reduction / representation of the data that facilitates comparisons
across tables, rather than just comparing each table with a single response of
interest. This richer scientific question motivates the development of methods
distinct from those used to analyze a single measurement type at a time.

For more concrete motivation, we will use data from the WELL-China study, which
is focused on the relationship between various indicators of wellness and
microbial community structure (todo: give reference). In this study, 1969
individuals\footnote{Though sampling is still ongoing.} underwent clinical
examinations, filled out wellness surveys (covering topics such as exercise,
sleep, diet, and mental health, for example), and provided stool samples, used
for 16s sequencing and metabolomic analysis. To date, 16s sequencing data is
available for 221 of these participants. Evidently, various interesting
relational questions can be investigated using this data source.

For the purpose of illustration, we focus on one relatively focused question
that can be addressed using this data: How is the distribution of lean and fat
mass across the body related to patterns of microbial abundance? The measurement
types most relevant in this analysis are (1) DEXA scans and (2) 16s sequencing.
DEXA scans use relative X-ray absorption to gauge the among of lean and fat body
mass within the region of the body being scanned. We will have access to these
lean and fat body mass measurements at several body sites -- arms, legs, trunk,
etc. -- along with related body type variables, like height, age, and android
and gynoid fat measurements. In total, there are 33 of these variables. 16s
sequencing is a technology for gauging the abundance of differnet bacterial
species in the gut by counting the alignments of reads to the 16s gene, a
component of all bacterial genomes with enough variation to allow discimination
between different individual species. We have counts associated with 2565
species across 181 genuses, though the vast majority are present in low
abundances.

This question of the relationship between lean and fat mass distribution
(informally, body type) and the microbiome follows up findings that certain
taxonomic groups are over or underrepresented as a function of an individual's
BMI or obesity \citep{ley2006microbial, turnbaugh2009core, ley2005obesity,
  ley2010obesity}. Further, since the distribution of fat is often more related
to underlying biological mechanisms than overall body mass
\citep{matsuzawa2008role}, and since this distribution is mediated by specific
metabolic pathways, there is reason to suspect that a joint analysis of DEXA and
16s microbial abundance data might yield a more complete view of the
relationship between the microbiome and body type (and the associated disease
risk factors).

\section{Classical multivariate methods}

Methods from classical multivariate statistics are a mainstay of single-table
microbiome data analysis, so it is natural to revisit this literature before
surveying extensions to the multitable setting. Here we describe a few of the
classically studied multitable methods that fit nicely into the modern
microbiome data analysis toolbox. We first describe a naive approach based on
Principal Components Analysis (PCA) -- naive because it lifts a single-table
method to the multiple table setting without any special considerations --
before studying approaches that directly characterize covariation across several
tables: Canonical Correlation Analysis (CCA), Multiple Factor Analysis (MFA),
and Principal Component Analysis with Instrumental Variables (PCA-IV).

The earliest multitable method (CCA) was published in 1936, where the motivating
data analysis problem was to relate prices of groups of commodities
\citep{hotelling1936relations}. There are two notable aspects of data analysis in
this classical paradigm which no longer hold in modern statistics:
\begin{itemize}
  \item Even when many samples could be collected, there were typically only a
    few features for each sample, and it was straightforwards to study all of
    them simultaneously. In the last few decades, it has become possible to
    automatically collect a large number of features for each sample.
  \item Before electronic computers had been invented, it was important that all
    statistical quantities be easy to calculate, typically necessitating
    analytical formulas for parameter estimates. This is no longer as important
    a limitation in an environment with richer computational resources.
\end{itemize}

These changes have motivated the need for high-dimensional methods and
facilitated the adoption of iterative, more computationally-intensive
approaches, respectively, some of which are described later in this review.

Nonetheless, it is worth reviewing these original approaches, both to understand
the context for many modern techniques, as well as to have an easy starting
point for practical data analysis. Indeed, these more established methods tend
to be the most readily available through statistical computing packages and can
provide a benchmark with which to compare more elaborate, modern methods.

\subsection{PCA}
\label{subsec:pca}

The simplest approach to dealing with multiple tables is to combine them into
one and apply a single-table method, for example, PCA. That is, write
\begin{align*}
X = \left[X^{(1)} \vert \dots \vert X^{(L)}\right] \in \reals^{n \times p},
\end{align*}
where $p = \sum_{l = 1}^{L}p_{l}$, and compute the SVD\footnote{An equivalent
  procedure is to eigenanalyze the empirical covariance matrix
  $\frac{1}{n}X^{T}X$.}, $X = UDV^{T}$. The $K$-principal component directions
are the first $K$ columns $v_{\cdot 1}, \dots, v_{\cdot K}$, while the
associated scores are $d_{1}u_{\cdot 1}, \dots, d_{K}u_{\cdot K}$.

While this does not account for the multitable structure of the data, it does
accomplish two goals,
\begin{itemize}
\item Through the principal component scores, it provides a visualization of the
  relationships between samples, based on all features.
\item Through the principal component directions, it gives a way of relating
  features within and across the multiple tables.
\end{itemize}

However, two drawbacks of this approach are worth noting,
\begin{itemize}
  \item It does not provide a summary of the relationship between the sets of
    variables defining the tables -- it can only relate pairs of
    variables. \label{bullet:pca_drawback_one}
  \item If some tables have many more variables than others, they can dominate
    the resulting ordination. \label{bullet:pca_drawback_two}
\end{itemize}

These limitations are addressed by CCA and MFA, discussed in Sections
\ref{subsec:cca} and \ref{subsec:mfa}, respectively.

Here we describe one geometric and one statistical motivation for PCA. The
geometric motivation is that, if each row $x_{i}$ of $X$ is viewed as a point in
$p$-dimensional space, then the principal component directions provide the best
$k$-dimensional approximation to the data, see Figure \ref{fig:pca-approx}.
Formally, recall that $VV^{T}x_{i}$ is the projection of $x_{i}$ onto the
subspace spanned by the columns of $V$. PCA identifies the orthogonal matrix $V
\in \reals^{p \times K}$ such that
\begin{align*}
\sum_{i = 1}^{n}\|x_{i} - VV^{T} x_{i}\|_{2}^{2}
\end{align*}
is minimized. The principal component scores are then the coordinates of the
projected points with respect to this subspace.

\begin{figure}
  \includegraphics[width=0.7\textwidth]{figure/pca/proj_plot_1}
  \caption{A geometric motivation for PCA. The first principal component spans
    the one-dimensional subspace $V$ minimizing the squared error between points
    and their projections, $\sum_{i = 1}^{n}\|\left(I -
    VV^{T}\right)x_{i}\|_{2}^{2}$ projections. The black points represent the
    scaled values for two of the body composition variables -- weight and total
    fat mass -- while the purple points are the projections onto the first
    principal component when only accounting for these two variables. Contrast
    this with Figures \ref{fig:pca-approx-2} and \ref{fig:pca-approx-3}, where
    variables other than weight and fat mass are taken into account.}
  \label{fig:pca-approx}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figure/pca/var_plot}
  \caption{A statistical motivation for PCA. Each histogram is computed from the
    linear combinations $\left(c^{j}x_{i}\right)$, for $c^{j}$ equal to the
    first and second principal components, the vector $\frac{1}{n}\mathbf{1}$,
    and a normalized random multivariate Gaussian. At least among these four
    vectors, the scores obtained from the first PC have the largest variance. It
    is a theorem that its variance is in fact maximal among all linear
    combinations with orthogonal vectors.}
  \label{fig:pca-var}
\end{figure}

The second interpretation is that PCA finds a low-dimensional representation of
the $x_{i}$ such that the resulting points have maximal variance. Qualitatively,
this is a desirable property, because it means that the simpler representation
preserves most of the variation present in the original data, see Figure
\ref{fig:pca-var}. In this figure, histograms of the scores associated with four
linear combinations of the original body composition measurements are displayed
side by side, to emphasize the fact that the linear combination $x^{T}v_{1}$
associated with the first principal component $v_{1}$ has the largest variance.
The comparison combinations are the second principal component, the mean across
all columns, and a random linear combination $\frac{c^{T}}{\|c\|} x$ where $c \sim
\Gsn\left(0, I_{p}\right)$.

Formally, suppose that the $x_{i}\in\reals^{p}$ are drawn
independently from some distribution $\P$, so that the variance is
$\Covsubarg{\P}{x_{i}} = \Sigma$. Consider an arbitrary linear combination of
$x_{i}$'s $p$ coordinates: $z_{i} := c^{T}x_{i}$ for some $c \in \reals^{p}$.
The first PCA direction gives the $c$ such that the variance of this coordinate,
$\Varsubarg{\P}{z_{i}} = c^{T}\Sigma c$, is maximal. The second direction gives
the linear combination that maximizes variance, subject to being orthogonal to
the first, and so forth.

While our description of the method of concatenating multiple tables into a
single one has focused on PCA, note that other methods could be applied instead.
For example, it is possible to define a new distance between samples as a
mixture of distances based on several tables. This can be useful if there are
different types of data across the different tables: Jaccard, $\chi^{2}$, and
euclidean distsances can be applied to binary, count, and real valued tables.
The combined distance can then be input into any distance-based single-table
procedure, like multidimensional scaling or hierarchical clustering. The primary
downside of this approach is that the resulting distance only allows a
comparison between samples, but not across features.

PCA is a very widely used technique, and some standard references include
\citep{friedman2001elements, mardia1980multivariate, pages2014multiple}.
Nonetheless, it is not ideal in the multitable setting. Its limitations prompt
us to begin our survey of multitable methods in earnest.

\subsubsection{Example}
\label{subsubsec:pca_example}

Figure \ref{fig:loadings} and \ref{fig:scores_weight} illustrate this approach
on body composition and bacterial abundance data from the WELL-China study.
Note that we have subsetted to only men, since men and women have very different
body compositions. Further, the 16s data have been variance stabilized according
the methodology proposed in \citep{Anders2010} and filtered to only those
amplicon sequence variants (ASVs) that are nonzero in at least 7\% of samples.

Figure \ref{fig:loadings} displays the loadings associated with this
combined-dataset PCA approach, where body composition (36 columns) and 16s
abundances (368 columns) were combined into one dataset (401 columns). Columns
associated with bacterial species are displayed as points, shaded by taxonomic
family, while columns associated with body composition variables are labeled
with text.

Most body composition variables lie on the top right, in a direction
approximately orthogonal to the main direction of variation among ASVs. Columns
that are highly correlated (e.g., right (R) and left (L) Trunk Fat Mass (FM)
have loadings nearly equal to one another. Age appears in generally negatively
correlated with the other, more weight oriented, variables. Among ASVs, the main
source of variation seems to be between Ruminococcaceae and Lachnospiraceae,
which appear overrepresented on the left and right, respectively.

To identify relationships between ASVs and body composition variables, it would
be of interest to isolate those ASVs with large contributions along the axis
defined by linking the center of the weight variables and the origin. Relatively
few such species stand out, though note that there is nothing in this
algorithm's objective that would seek covariation across tables directly, so the
fact that such associations seem weak with respect to the top two principal
components does not mean such relationships do not exist.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/pca/loadings}
  \caption{The loadings obtained by applying PCA to the combined body
    composition and microbial abundance data set. \label{fig:loadings} }
\end{figure}

We can study individual samples with respect to these loadings, by plotting
their projections onto the top two principal components. This is the content of
Figures \ref{fig:scores_weight} and \ref{fig:scores_rl_ratio}. These figures
display samples in the same positions, but shaded by weight and Ruminococcaceae
/ Lachnospiraceae ratio, respectively. This shading confirms the observations
from the loadings directly using observed data. Indeed, the increasing weight
among samples in the top right of Figure \ref{fig:scores_weight} exactly
corresponds to the fact that weight related variables lie in the top right in
Figure \ref{fig:loadings}. A left-to-right Ruminococcaceae /
Lachnospiraceae differentiation is visible in Figure \ref{fig:scores_rl_ratio},
which is consistent with the loadings, though we also observe a few samples with
substantially higher Ruminoccocus ratios.

In this approach, the loadings provide a description of the relationship between
variables across data sets. Further, scores summarize variation in samples
across multiple data sets. In this light, this simple heuristic is a natural
first step in analyzing multiple table data. However, considering the difficulty
in directly interpreting the covariation across data sets, as well as the
method's failure to use any sense of covariation in the dimensionality
reductions trategy, suggests that this method should not be the last step of an
analysis workflow. Nevertheless, we now have a baseline with which to compare
the more elaborate methods of subsequent sections.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/pca/scores_weight}
  \caption{The scores resulting from PCA applied to the combined body
    composition and microbial abundance data set.\label{fig:scores_weight} }
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/pca/scores_rl_ratio}
  \caption{The same scores as Figure \ref{fig:scores_weight}, but shaded now by
    the ratio of Ruminococcaceae over Lachnospiraceae relative abundances. The
    slight increase in Ruminococcaceae from left to right is consistent with the
    loadings observed in Figure \ref{fig:loadings}.\label{fig:scores_rl_ratio} }
\end{figure}

\subsection{CCA}
\label{subsec:cca}

CCA is a close relative of PCA that designed to compare sets of features across
tables. Like PCA, it provides low-dimensional representations of samples, but it
also allows comparisons at the table level. Suppose for now that there are only
two tables of interest, $X \in \reals^{n \times p_{1}}$ and $Y \in \reals^{n
  \times p_{2}}$. Let $\hat{\Sigma}_{XX}, \hat{\Sigma}_{YY}$, and
$\hat{\Sigma}_{XY}$ be the associated covariance estimates. Take the SVD,
$\hat{\Sigma}_{XX}^{-\frac{1}{2}}\hat{\Sigma}_{XY}\hat{\Sigma}_{YY}^{-\frac{1}{2}}
= \tilde{U}D\tilde{V}^{T}$. The canonical correlation directions associated with
the two tables are $u_{k} = \Sigma_{XX}^{-\frac{1}{2}}\tilde{u}_{k} \in
\reals^{p_{1}}$ and $v_{k} = \Sigma_{YY}^{-\frac{1}{2}}\tilde{v}_{k} \in
\reals^{p_{2}}$. These directions give two sets of low-dimensional
representations for each sample, one for each table: $z_{k}^{(1)} = Xu_{k} \in
\reals^{n}$ and $z_{k}^{(2)} = Yv_{k} \in \reals^{n}$. If the two tables are
closely related, then the $z_{k}^{(1)}$ and $z_{k}^{(2)}$ will be very
correlated. The singular values $d_{k}$ are called the canonical correlation
coefficients. Like the eigenvalues in PCA, they characterize the amount of
covariation across tables that can be captured by each additional pair of
directions.

As in PCA, there are many ways to view this procedure; here we discuss
geometric, statistical, and probabilistic interpretations. Unlike the geometric
interpretation for PCA, the geometric interpretation for CCA identifies point
locations with features, not samples. Specifically, the columns of $X$ and $Y$
are thought of as points in $\reals^{n}$. Consider two subspaces defined as the
spans of the columns of $X$ and $Y$, respectively. These subspaces correspond to
the linear combinations of features within each table. Place two ellipses on the
respective subspaces, centered at the origin and with size and shape depending
on the within table covariances $\hat{\Sigma}_{XX}$ and $\hat{\Sigma}_{YY}$. The
first canonical correlation directions are the pair of points, one lying on each
ellipse, such that the angle from the origin to those two points is smallest. In
this sense, it finds a pair of variance-constrained linear combinations of
features within the two tables so that the two combinations appear ``close'' to
one another. The second pair of canonical correlation directions identify a pair
of points with similar interpretation, except they are required to be orthogonal
to the first pair, with respect to the inner product induced by the covariances
in each table.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figure/cca/geometric}
  \caption{The geometric representation of CCA, on a toy subset of the WELL-China
    data, to facilitate visualization. The three axes correspond correspond to
    three subjects, and the text labels indicate the positions of (scaled) features with
    respect to the samples. Features from the 16s table are shaded in blue,
    while body composition variables are orange. The subspaces spanned by the
    three features in each table are indicated by a grid of points lying along
    them, shaded in the same color as the measurement type they correspond to.
    We only display three features so that the table subspaces do are not simply
    all of $\reals^{3}$. A set of points lie along ellipses that are unit balls
    with respect to each data set's covariance matrix, estimated using all
    samples (not just the three displayed). Two arrows are the first CCA
    directions associated with the two measurement types. Evidently, the
    correlation between these measurement types, restricted to these few samples
    and features, is quite high, since the arrows point in nearly the same
    direction.
    \label{fig:cca_geometric} }
\end{figure}

For a statistical interpretation, the idea of CCA is to find the low-dimensional
representations of the two tables with maximal covariance -- this is analogous
to the maximum variance interpretation. Formally, let $x_{i}$ and $y_{i}$
samples from $\P^{\X}$ and $\P^{\Y}$. The rows of the two tables are imagined to
be i.i.d. draws from $\P^{\X\Y}$. Consider arbitrary linear combinations
$z_{i}^{(1)}\left(u\right) = u^{T} x_{i}$ and $z_{i}^{(2)}\left(v\right) =
v^{T}y_{i}$ of samples from the two tables. The first pair of CCA directions
$u_{1}^{\ast}$ and $v_{1}^{\ast}$ are chosen to optimize
\begin{align}
  \maximize_{u \in \reals^{p_{1}}, v \in \reals^{p_{2}}}
  &\Covsubarg{\P^{\X\Y}}{z_{i}^{(1)}\left(u\right),
    z_{i}^{(2)}\left(v\right)} \label{eq:cancor_optim} \\
\text{subject to } &\Varsubarg{\P^{\X}}{z_{i}^{(1)}\left(u\right)} = 1 \nonumber \\
&\Varsubarg{\P^{\Y}}{z_{i}^{(2)}\left(v\right)} = 1. \nonumber
\end{align}
To produce subsequent directions, the same optimization is performed, but with
the additional constraint that the directions must be orthogonal to all the
previous directions identified for that table. Of course, in actual
applications, we estimate these covariances and variances empirically.

This perspective makes it easy to derive the algorithm given at the start of
this section. The empirical version of the optimization problem
\ref{eq:cancor_optim} is
\begin{align}
  \maximize_{u \in \reals^{p_{1}}, v \in \reals^{p_{2}}}
  &u^{T}\hat{\Sigma}_{XY}v \label{eq:cancor_optim_emp}\\
  \text{subject to } & u^{T}\hat{\Sigma}_{XX}u = 1 \nonumber \\
  & v^{T}\hat{\Sigma}_{YY}v = 1. \nonumber
\end{align}

Consider the whitened data, $\tilde{u} = \hat{\Sigma}_{XX}^{\frac{1}{2}}u$ and
$\tilde{v} = \hat{\Sigma}_{YY}^{\frac{1}{2}}v$. The
optimization \label{eq:cancor_emp} can now be expressed as
\begin{align}
  \maximize_{\tilde{u} \in \reals^{p_{1}}, \tilde{v} \in
    \reals^{p_{2}}}
  &\tilde{u}^{T}\hat{\Sigma}_{XX}^{-\frac{1}{2}}\hat{\Sigma}_{XY}\hat{\Sigma}_{YY}^{-\frac{1}{2}}\tilde{v} \label{eq:cancor_trans} \\
\text{such that } & \|\tilde{u}\|^{2}_{2} = 1 \\
&\|\tilde{v}\|_{2}^{2} = 1.
\end{align}
The optimal $\hat{\tilde{u}}_1$ and $\hat{\tilde{v}}_1$ for this problem are
well known -- they're exactly the first left and right eigenvectors of
$\hat{\Sigma}_{XX}^{-\frac{1}{2}}\hat{\Sigma}_{XY}\hat{\Sigma}_{YY}^{-\frac{1}{2}}
= \tilde{U}D\tilde{V}^{T}$, respectively. The argument is standard\footnote{See
  \citep{mardka1980multivariate}, for example.}, but we include it for
completeness.

Let $\xi$ and $\nu$ be potential maximizers of length one. We can find $w_{u}$
and $w_{v}$ such that $\xi = \tilde{U}w_{u}, \nu = \tilde{V}w_{v}$, since
$\tilde{U}$ and $\tilde{V}$ are both orthonormal bases. Since they are length
one, $1 = \|\xi\|^{2}_{2} = w_{u}^{T}\tilde{U}^{T}\tilde{U}w_{u} =
\|w_{u}\|_{2}^{2}$ and similarly $\|w_{v}\|_{2}^{2} = 1$. The objective
\ref{eq:cancor_trans} can be bounded by
\begin{align*}
\xi^{T}\hat{\Sigma}_{XX}^{-\frac{1}{2}}\hat{\Sigma}_{XY}\hat{\Sigma}_{YY}^{-\frac{1}{2}}\nu
&= w_{u}^{T}\tilde{U}^{T}\tilde{U}D\tilde{V}^{T}\tilde{V}w_{v} \\
&= w_{u}^{T}Dw_{v} \\
&= \sum_{k = 1}^{p_{1} \wedge p_{2}} d_{k}w_{uk}w_{vk} \\
&\leq d_{1} \sum_{k = 1}^{p_{1} \wedge p_{2}} w_{uk}w_{vk} \\
&\leq d_{1} \|w_{u}\|\|w_{v}\| = d_{1},
\end{align*}
and this maximum is attained when $w_{u}$ and $w_{v}$ both put all their weight
on the first coordinate, that is $\xi = \tilde{u}_{1}$ and $\nu =
\tilde{v}_{1}$. For subsequent directions, we repeat the argument but require
that $w_{u}$ and $w_{v}$ have zero weight on the first columns of $\tilde{U}$
and $\tilde{V}$.

To recover the solutions to the original problem, we reverse the original
variable transformation, yielding optimal $\hat{u}_{1} =
\Sigma_{XX}^{-\frac{1}{2}}\hat{\tilde{u}}_{1}$ and $\hat{v}_{1} =
\Sigma_{YY}^{-\frac{1}{2}}\hat{\tilde{v}}_{1}$.

A probabilistic interpretation of this procedure views it as estimating the
factors in an implicit latent variable model. In particular,
\citep{bach2005probabilistic} supposes that $x_{i}$ and $y_{i}$ are drawn i.i.d.
from the model,
\begin{align*}
  \xi_{i} := \left(\xi_{i}^{s}, \xi_{i}^{X}, \xi_{i}^{Y}\right) &\sim
  \Gsn\left(0, I_{d}\right) \\
  x_{i} \vert \xi_{i} &\sim \Gsn\left(W_{X}\xi_{i}^{s} + B_{X}\xi_{i}^{X} + \mu_{X},
  I_{d} \right) \\
  y_{i} \vert \xi_{i} &\sim \Gsn\left(W_{Y}\xi_{i}^{s} +
  B_{Y}\xi_{i}^{Y} + \mu_{Y}, I_{d}\right)
\end{align*}
That is, each sample is associated with a $d$-dimensional latent variable
$\xi_{i}$, drawn from a spherical normal prior. A few of the coordinates of
these latent variables contribute to shared structure, through $W_{X}$ and
$W_{Y}$. The remaining coordinates model table-specific structure, through
$B_{X}$ and $B_{Y}$. It can be shown that the posterior expectations of the
latent $\xi_{i}^{s}$ (assumed $\in \reals^{K}$) given the observed tables must
lie on the subspace defined by the CCA directions. More precisely,
\begin{align*}
  \Earg{\xi_{i} \mid x_{i}^{(1)}} \in \text{span}\left(z_{1}^{(1)},
    \dots, z_{K}^{(1)}\right),
\end{align*}
and
\begin{align*}
  \Earg{\xi_{i} \mid x_{i}^{(2)}} \in \text{span}\left(z_{1}^{(2)},
    \dots, z_{K}^{(2)}\right).
\end{align*}

Finally, we observe that the logic for CCA generalizes to an arbitrary number
$L$ of tables, by summing all pairwise covariances. That is, the instead of
finding directions $c_{k}^{(1)}$ and $c_{k}^{(2)}$ maximizing
$\Covsubarg{\P^{(1)}, \P^{(2)}}{c_{k}^{(1)T}x_{i}^{(1)},
  c_{k}^{(2)T}x_{i}^{(2)}}$ subject to normalization and orthogonality
constraints, we seek directions $c_{k}^{(1)}, \dots, c_{k}^{(L)}$ that maximize
the sum of cross-covariances $\sum_{l,l^{\prime} = 1}^{L} \Covsubarg{\P^{(l)},
  \P^{(l^{\prime})}}{c_{k}^{(l) T}x_{i}^{(l)},
  c_{k}^{(l^{\prime})}x_{i}^{(l^{\prime})}}$.

\subsubsection{Example}
\label{subsubsec:cca_example}

We next apply CCA to the WELL-China body composition and microbiome data sets,
with particular interest in how results compare with those of
Section \ref{subsubsec:pca_example}. To this end, we provide analogous loading and score
plots in Figures \ref{fig:cca_loadings} through \ref{fig:cca_scores_rl_ratio}.
However, note that the data are \textit{not} quite the same between the two
analysis -- we have filtered down to sequence passing a filter\footnote{Presence
  in at least half of samples}, which reduces the number of sequences to 57,
from 2565. This very aggressive filtering is necessary because CCA requires
estimation of covariances matrices $\Sigma_{XX}, \Sigma_{XY}$, and
$\Sigma_{YY}$, which is impossible for $p > n$ and highly unstable when $p$ a
large fraction of $n$. Besides this stronger filtering, all preprocessing steps
remain the same as in Section \ref{subsubsec:pca_example}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/CCA/loadings}
  \caption{The loadings obtained by applying CCA to the combined body
    composition and microbial abundance data set. \label{fig:cca_loadings} }
\end{figure}

Figure \ref{fig:cca_loadings} provides the analog of CCA loadings. To be
precise, let $X \in \reals^{102\times 33}$ be the matrix of body composition
measurements and $Y \in \reals^{102 \times 57}$ be the variance stabilized
microbial abundances. As before, write $\hat{u}_{k} \in \reals^{33}, \hat{v}_{k}
\in \reals^{57}$ for the $k^{th}$ canonical correlation directions. Then, Figure
\ref{fig:cca_loadings} displays text labels for column $j$ of the body
composition variables at location $\times \left(u_{j1}, u_{j2}\right)_{j =
  1}^{33}$ and shaded points for the $j^{th}$ species at position
$\left(\hat{v}_{j1}, \hat{v}_{j2}\right)_{j = 1}^{57}$.

As in the joint PCA, we find that the groups of variables occupy separates
spaces. Our interpretation is that sequences further to the left are correlated
with the body variables further to the left, which are all in some ay variants
of weight. Note that age is negatively correlated with weight, which is why it
appears on the opposite end. Among the abundant species that remain, there is
limited clustering according to taxonomic group, though the Bacteroideceae and
Ruminoccocus do appear restricted to the bottom and top right, respectively.
Note that the third CCA axis (the size of plotted text and points) distinguishes
lean and fat mass variables.

In Figure \ref{fig:cca_scores_linked} we plot the corresponding scores. Note
that in CCA, there are two sets of scores for each $k$, the $X\hat{u}_{k}$ and
$Y\hat{v}_{k}$. Indeed, the CCA objective finds directions that maximizes the
correlation between these scores. In this figure, we display both sets of
scores, colored differently according to the table to which they are associated
with. The pairs of scores for each individual sample are drawn with small links.
Since most links are relatively short, linear combinations of the two tables
could be found that optimized the objective -- indeed, the top two canonical
correlations are 0.984 adn 0.961. However, some caution is necessary here, and a
more honest evaluation would be based scores obtained by projecting new samples
onto the original CCA directions. This is especially important in this nearly
high-dimensional setting, where covariance estimation may be unreliable.

Aside from the fact that samples appear as pairs, interpretation proceeds as in
the PCA biplot. For example, Figures \ref{fig:cca_scores_weight} and
\ref{fig:cca_scores_rl_ratio} display the samples shaded in by weight and
Ruminococcaceae vs. Lachnospiraceae ratios, respectively, as in Figures
\ref{fig:scores_weight} and \ref{fig:cca_scores_rl_ratio}. The association
between these variables and the sample positions is not as strong as when
performing PCA on the combined table. This is to be expected, however, as PCA
maximizes variance without any requirements on covariance, and the body
composition table alone has a large portion of its variance related to weight.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/cca/scores_linked}
  \caption{The scores resulting from CCA applied to the combined body
    composition and microbial abundance data set. Each measurement type provides
    a different set of scores for the observed samples, and the similarity
    between the sets of scores is reflected in the CCA
    objective.\label{fig:cca_scores_linked}}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/cca/scores_weight}
  \caption{The scores resulting from CCA applied to the combined body
    composition and microbial abundance data set, shaded in by
    weight.\label{fig:cca_scores_weight} }
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/cca/scores_rl_ratio}
  \caption{The same scores as Figure \ref{fig:cca_scores_weight}, but shaded now by
    the ratio of Ruminococcaceae over Lachnospiraceae relative
    abundances. \label{fig:cca_scores_rl_ratio} }
\end{figure}

\subsection{Co-Inertia Analysis}

Co-Inertia Analysis (CoIA) emerged in ecology to facilitate analysis of
variation in species abundance as a function of environmental conditions
\cite{doledec1994co}. It can be viewed as a slight modification -- perhaps even
a simplification -- of CCA. Again, we seek sets of orthonormal directions
$\left(u_{k}\right)_{k = 1}^{K}$ and $\left(v_{k}\right)_{k = 1}^{K}$ such that
the associated projections $Xu_{k}$ and $Yv_{k}$ explain most of the covariation
between the tables. Unlike CCA, CoIA finds directions that maximize the
covariance -- not the correlation -- between scores,
\begin{align*}
\maximize_{u \in \reals^{p_{1}}, v \in \reals^{p_{2}}} &u^{T}X^{T}Yv \\
\text{ such that}\medspace &\|u\| = 1\\
&\|v\| = 1,
\end{align*}
and subsequent directions by the same optimization, with the
additional constraint that they are orthogonal to the previously
derived directions.

The only difference with the objective in equation \ref{eq:cancor_optim_emp} is
that norm constraint is imposed on $u$ and $v$ directly, rather than their
transformations $\Sigma_{XX}^{\frac{1}{2}}u$ and $\Sigma_{YY}^{\frac{1}{2}}v$.
It is in this sense that the CCA objective maximizes the correlation between
scores, while CoIA maximizes the covariance.

The solutions $\left(u_{k}\right)_{k = 1}^{K}$ and $\left(v_{k}\right)_{k =
  1}^{K}$ can be obtained as the first $K$ left and right eigenvectors from the
SVD of $X^{T}Y$, as opposed to the first $K$ generalized eigenvectors, as in
CCA, by essentially the same argument as in CCA.

\subsubsection{Example}
\label{subsubsec:coia_example}

We apply CoIA to the same data as used in Section \ref{subsubsec:cca_example},
as CoIA also needs to estimate the covariance between tables, which is difficult
when the number of ASVs is large. The loadings are displayed in Supplementary
Figure \ref{fig:coia_loadings}, and they are similar to those in Figure
\ref{fig:cca_loadings}. However, the associated scores are quite
different than those found using CCA, consider Figure
\ref{fig:coia_scores_weight}, which shades samples by weight, or
Supplemental Figure \ref{fig:coia_scores_rl_ratio}, which shades them by
Ruminococcaceae / Lachnospiraceae ratios. We find that the scores are not nearly as
closely aligned as they are for CCA (see Figure \ref{fig:cca_scores_weight}), but
that they are more strongly associated with variation in weight, as in the
concatenated-PCA result of Figure \ref{fig:scores_weight}. It is not
clear whether this phenomena -- the CoIA scores being more similar to those from
PCA than CCA -- holds in general, or what about the change in inner products
between CoIA and CCA is responsible for this difference.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/coia/scores_weight}
  \caption{The normalized scores from each table, displayed
    simultaneously, as obtained by CoIA. This is the analog of Figure
    \ref{fig:cca_scores_weight} from CCA. \label{fig:coia_scores_weight} }
\end{figure}


\subsection{MFA}
\label{subsec:mfa}

MFA gives an alternative approach to producing scores and relating features
across multiple tables\citep{pages2014multiple}. It can be understood as a
reweighted version of the concatenated PCA described in Section \ref{subsec:pca}
that reweights tables in a way that prevents any one table from dominating the
resulting ordination Specifically, MFA is a concatenated PCA on the matrix
\begin{align*}
X := \left[\frac{1}{\lambda_{1}\left(X^{(1)}\right)}X^{(1)} \vert \dots
  \vert \frac{1}{\lambda_{1}\left(X^{(L)}\right)}X^{(L)}\right],
\end{align*}
which reweights each table by its largest eigenvalue. This procedure is the
multitable analog of the standard practice of standardizing variables before
performing PCA.

The resulting MFA directions and scores can be interpreted in the same
way as those from PCA -- the MFA directions still specify the
relationship between measured features, and the position of each
sample's projection describes the relative weight of each feature for
that sample. Moreover, MFA gives a way of comparing full tables to
each other, called a ``canonical analysis'' \cite{pages2004multiple}. A
$K$-dimensional representation of the $l^{th}$ group is given by
\begin{align*}
\left[\mathcal{L}\left(z_{1}, X^{(l)}\right), \dots,
  \mathcal{L}\left(z_{K}, X^{(l)}\right)\right]
\end{align*}
where $z_{k} = d_{k}u_{k} \in \reals^{n}$ is the $k^{th}$ column of principal
component scores and
\begin{align*}
  \mathcal{L}\left(z_{k}, X^{(l)}\right) =
  \frac{\lambda_{k}\left(X\right)}{\lambda_{1}\left(X^{(l)}\right)}\tr\left(X^{(l)}X^{(l)
      T} z_{k}z_{k}^{T}\right) =
  \frac{\lambda_{k}\left(X\right)}{\lambda_{1}\left(X^{(l)}\right)}\|X^{(l)
  T} z_{k}\|^{2}_{2}
\end{align*}
is a measure of aggregate similarity between the coordinates in the $l^{th}$
table and the $k^{th}$ column of scores. According to this definition, if the
samples, as represented by the $l^{th}$ table, have high correlation with the
$k^{th}$ dimension of scores, then the canonical analysis display positions the
$l^{th}$ table far in the $k^{th}$ direction.

\subsection{PCA-IV}

PCA-IV adapts the dimension reduction ideas of PCA to the multivariate
regression setting \citep{rao1964use}. It can also be viewed as a version of PCA
that chooses a dimension reduction of $X$ depending on its ability to predict
$Y$. In this sense, it anticipates methods like Partial Least Squares (PLS),
Canonical Correspondence Analysis (CCpnA), the Curds \& Whey procedure, and
various approaches to multitask learning, which are described in Sections
\ref{subsec:PLS}, \ref{subsec:canonical-correspondence}, \ref{subsec:cw}, and
\ref{subsec:multitask}, respectively.

Formally, suppose we are predicting $y_{i} \in \reals^{p_{1}}$ from $x_{i} \in
\reals^{p_{2}}$. Since $p_{2}$ may be large, it might be useful to work with a
lower-dimensional representation $z_{i} = V^{T}x_{i} \in \reals^{K}$, that is
potentially more interpretable and still as (or more) predictive of $y_{i}$. As
in PCA, we require that $V$ be orthonormal.

The criterion that PCA-IV uses to identify the weights $V$ and scores $Z$
mirrors the maximum variance criterion for PCA. Instead of choosing $V$ to
maximize the variance of the $z_{i}$, we choose it to minimize the residual
covariance of $y_{i}$ given $z_{i}$. That is, suppose that $y_{i}$ and $x_{i}$
are jointly normal with mean 0 and covariance
\begin{align*}
\Var_{\P}\begin{pmatrix}y_{i} \\ x_{i}\end{pmatrix} &=
\begin{pmatrix}
  \Sigma_{YY} & \Sigma_{YX} \\
  \Sigma_{XY} & \Sigma_{XX}
\end{pmatrix}.
\end{align*}

If $z_{i} = V^{T}x_{i}$, then the joint covariance of $y_{i}$ and $z_{i}$ is
\begin{align*}
  \Var_{\P}\begin{pmatrix} y_{i} \\ z_{i} \end{pmatrix} &=
  \begin{pmatrix}
    \Sigma_{YY} & \Sigma_{YX}V \\
    V^{T}\Sigma_{XY} & V^{T}\Sigma_{XX}V
  \end{pmatrix},
\end{align*}
and the residual covariance of $x_{i}^{(1)}$ given $z_{i}$ is
\begin{align}
  \Sigma_{YY} -
  \Sigma_{YX}V\left(V^{T}\Sigma_{XX}V\right)^{-1}V^{T}\Sigma_{XY}. \label{eq:pca_iv_resid_cov}
\end{align}
\citep{rao1964use} uses the trace to measure the ``size'' of this matrix. The
true population covariances are unknown to us, so we replace them by their
empirical estimates. The formal optimization for PCA-IV then becomes
\begin{align}
  \minimize_{V\in \reals^{p_{2} \times K} \text{ orthonormal}}
  \tr\left(\hat{\Sigma}_{YY} -
  \hat{\Sigma}_{YX}V\left(V^{T}\hat{\Sigma}_{XX}V\right)^{-1}V^{T}\hat{\Sigma}_{XY}\right) \label{eq:pca_iv_obj},
\end{align}
or, equivalently,
\begin{align}
  \maximize_{V\in \reals^{p_{2} \times K} \text{ orthonormal}}
  \tr\left(\hat{\Sigma}_{YX}V\left(V^{T}\hat{\Sigma}_{XX}V\right)^{-1}V^{T}\hat{\Sigma}_{XY}\right) \label{eq:pca_iv_obj_2},
\end{align}

The optimal $V$ are the top $K$ generalized eigenvectors of
$\hat{\Sigma}_{XY}\hat{\Sigma}_{YX}$ with respect to $\hat{\Sigma}_{XX}$, that
is, the orthonormal set of $\left(v_{k}\right)$ satisfying
\begin{align*}
\hat{\Sigma}_{XY}\hat{\Sigma}_{YX}v_{k} &= \lambda_{k}
\hat{\Sigma}_{XX}v_{k}, \text{for } $k = 1, \dots, K$
\end{align*}
or more concisely,
\begin{align*}
\hat{\Sigma}_{XY}\hat{\Sigma}_{YX}V &= \left( \lambda_{1}
  \hat{\Sigma}_{XX}v_{1} \vert \dots \vert
  \lambda_{k}\hat{\Sigma}_{XX}v_{k}\right) =
\hat{\Sigma}_{XX}V\Lambda,
\end{align*}
where $\Lambda = \diag\left(\lambda_{k}\right) \in \reals^{K \times K}$. In
particular, $\hat{\Sigma}_{XY}\hat{\Sigma}_{YX}$ has generalized
eigendecomposition $\hat{\Sigma}_{XY}\hat{\Sigma}_{YX} = \hat{\Sigma}_{XX}
V\Lambda V^{T}$. A derivation for why this choice is optimal is provided in
Supplemental Section \ref{subsec:pca_iv_derivation}.

For a geometric interpretation of PCA-IV, view each column $y_{\cdot j}$ in $Y$
and $x_{\cdot j}$ in $X$ as a point in $\reals^{n}$. Assuming $X$ and $Y$ are
full rank, the collections $\left(y_{ \cdot j}\right)$ and $\left(x_{\cdot
  j}\right)$ span $p_{1}$ and $p_{2}$-dimensional subspaces. A set of
independent regressions of $X$ onto the $y_{\cdot j}$ projects the $y_{\cdot j}$
onto the span of $\left(x_{\cdot j}\right)$, and the residuals are the distance
to this span. The PCA-IV procedure is an attempt to find a further
$K$-dimensional subspace within the span of the $\left(x_{\cdot j}\right)$ such
that the residuals of the regressions from $x_{\cdot j}^{(1)}$ onto this further
subspace is not much worse. This is displayed in Figure
\ref{fig:pca_iv_geometry}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{figure/pca_iv_geometry}
  \caption{A geometric view of PCA-IV. The columns of the response $Y$ are views
    as $n$-dimensional vectors. The grey plane is the span of
    $X$. Multivariate OLS simply projects the columns of $Y$ onto the plane,
    while PCA-IV searches for a further subspace $V$ on which to project all
    responses. \label{fig:pca_iv_geometry} }
\end{figure}

Indeed, write the usual estimates for the covariance matrices of interest,
\begin{align*}
  \hat{\Sigma}_{YY} = \frac{1}{n}Y^{T}Y \\
  \hat{\Sigma}_{YX} = \frac{1}{n}Y^{T}X \\
  \hat{\Sigma}_{XX} = \frac{1}{n}X^{T}X
\end{align*}
and observe that the residual covariance of equation \ref{eq:pca_iv_resid_cov}
can be expressed
\begin{align*}
  &\frac{1}{n}\left[Y^{T}Y -
    Y^{T}XV\left(V^{T}X^{T}XV\right)^{-1}V^{T}X^{T}Y\right] \\
  = &\frac{1}{n}\left[Y^{T}Y^{(T)} - Y^{T}Z\left(Z^{T}Z\right)^{-1}Z^{T}Y\right] \\
  = &Y\left(I - P_{Z}\right)Y,
\end{align*}
where $P_{Z} = Z\left(Z^{T}Z\right)^{-1}Z^{T}$ is the projection operator onto
the columns of $Z$. Minimizing the trace of this matrix is equivalent to
minimizing
\begin{align*}
  \tr\left(Y^{T}\left(I - P_{Z}\right)Y\right) &=
  \tr\left(Y^{T}\left(I - P_{Z}\right)^{T}\left(I -
      P_{Z}\right)Y\right) \\
  &= \|\left(I - P_{Z}\right)Y\|_{F}^{2} \\
  &= \sum_{j = 1}^{p_{1}}\|\left(I - P_{j}\right)y_{\cdot j}\|^{2}_{2},
\end{align*}
which is exactly the sum of squared residuals from the columns of $Y$ onto the
span of the PCA-IV subspace, justifying geometric picture.

\bibliographystyle{plainnat}
\bibliography{refs.bib}

\section{Appendix}

\subsection{Additional figures}

\begin{figure}
  \includegraphics[width=0.7\textwidth]{figure/pca/proj_plot_2}
  \caption{This is the analog of Figure \ref{fig:pca-approx} in the case that
    PCA is run on all body composition variables, rather than just Weight and
    Fatmass. That is, we project the original values for these two features
    onto the top two PCs obtained from a PCA on all body composition variables.
    Since the PCA is working in a large space, the projected points are
    generally not too far from their original positions. However, note that one
    outlier on the far right is projected into the bulk of points in the center
    -- the variation coming from this one point is too specific to be preserved
    by PCA.
  \label{fig:pca-approx-2}}
\end{figure}

\begin{figure}
  \includegraphics[width=0.7\textwidth]{figure/pca/proj_plot_3}
  \caption{Here we perform the same procedure as in Figure
    \ref{fig:pca-approx-2}, except instead of projecting onto the top two PCs,
    we project onto only the top PC. The main point is that, while in two
    dimensions (Figure \ref{fig:pca-approx}), the behavior of the projection is
    easy to understand in terms of orthogonal errors, the corresponding
    orthogonal projection in higher dimensions is more complex.
  \label{fig:pca-approx-3}}
\end{figure}

\begin{figure}[ht]
  \centering \includegraphics[width=\textwidth]{figure/coia/loadings}
  \caption{These are the loadings obtained from CoIA, which are analogous to
    those obtained from concatenated PCA (Figure \ref{fig:loadings}) and CCA
    (Figure \ref{fig:cca_loadings}). \label{fig:coia_loadings} }
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure/coia/scores_rl_ratio}
  \caption{These the same CoIA scores as in Figure \ref{fig:coia_scores_weight},
    but shaded instead by Ruminococcaceae / Lachnospiraceae ratio, as in Figures
    \ref{fig:scores_rl_ratio} and
    \ref{fig:cca_scores_rl_ratio}. \label{fig:coia_scores_rl_ratio} }
\end{figure}

\subsection{Derivation details for PCA-IV}
\label{subsec:pca_iv_derivation}

In this section, we provide the argument for why the generalized
eigendecomposition $\hat{\Sigma}_{XY}\hat{\Sigma}_{YX} =
\hat{\Sigma}_{XX}V\Lambda V^{T}$ provides the optimal $V$ used in PCA-IV.

First consider $k = 1$. Then, for any $\tilde{v}$, the objective
\ref{eq:pca_iv_obj_2} has the form
\begin{align}
  \tr\left(\hat{\Sigma}_{YX}\tilde{v}\left(\tilde{v}\hat{\Sigma}_{XX}\tilde{v}\right)^{-1}
  \left(\hat{\Sigma}_{YX}\tilde{v}\right)^{T}\right) &=
  \frac{\tilde{v}^{T}\Sigma_{XY}\Sigma_{YX}\tilde{v}}{\tilde{v}^{T}\Sigma_{XX}\tilde{v}} \label{eq:gev_opt_1}\\
  &= \frac{\tilde{w}^{T}\Sigma_{XX}^{-\frac{1}{2}}\Sigma_{XY}\Sigma_{YX}\Sigma_{XX}^{-\frac{1}{2}}\tilde{w}}{\|\tilde{w}\|_{2}^{2}}, \label{eq:gev_opt_2}
\end{align}
where we change variables $\tilde{w} = \Sigma_{XX}^{\frac{1}{2}}\tilde{v}$. But
to maximize \ref{eq:gev_opt_2}, just choose $\tilde{w}$ to be the top
eigenvector of
$\Sigma_{XX}^{-\frac{1}{2}}\Sigma_{XY}\Sigma_{YX}\Sigma_{XX}^{-\frac{1}{2}}$,
which implies that $\tilde{v}$ is the top generalized eigenvector of
$\Sigma_{XY}\Sigma_{YX}$ with respect to $\Sigma_{XX}$. Indeed, in this case,
\begin{align*}
  \Sigma_{XY}\Sigma_{YX}\tilde{v}
  &=\Sigma_{XY}\Sigma_{YX}\Sigma_{XX}^{-\frac{1}{2}}\tilde{w} \\
  &= \Sigma_{XX}^{\frac{1}{2}} \Sigma_{XX}^{-\frac{1}{2}}\Sigma_{XY}\Sigma_{YX} \Sigma_{XX}^{-\frac{1}{2}}\tilde{w}\\
  &= \Sigma_{XX}^{\frac{1}{2}}\lambda_{1}\tilde{w} \\
  &= \lambda_{1}\Sigma_{XX}\tilde{v}.
\end{align*}

Hence, in the case $K = 1$, the criterion is maximized by the top generalized
eigenvector. For larger $K$, recall that the problem of maximizing
$\frac{v^{T}Av}{\|v\|^{2}}$ over $v$ subject to being orthogonal to the first $K
- 1$ eigenvectors of $A$ is solved by the $K^{th}$ eigenvector of $A$, and
applying this fact in step \ref{eq:gev_opt_2} of the argument above gives the
result for general $K$.

\end{document}
